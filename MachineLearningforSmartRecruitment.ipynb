{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMART AGENT RECRUITMENT\n",
    "\n",
    "#Sami Akhtar\n",
    "\n",
    "#ANALYSIS AND DESCRIPTION\n",
    "\n",
    "#This is a classification machine learning problem to identify the best agents / applicants, for a Financial Distribution company, who will be able to source business for the company within 3 months post their 7 day corporate training.\n",
    "\n",
    "\n",
    "#I have made the predictions using LightGBM. Other models like XGBoost and AdaBoost was also used for experimentation.\n",
    "\n",
    "!pip install lightgbm\n",
    "!pip install xgboost\n",
    "\n",
    "#Importing Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "__Loading Datasets__\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()\n",
    "\n",
    "test.head()\n",
    "\n",
    "print(\"Train Dataset has {} records and {} features\".format(train.shape[0], train.shape[1]))\n",
    "print(\"Test Dataset has {} records and {} features\".format(test.shape[0], test.shape[1]))\n",
    "\n",
    "train.info()\n",
    "\n",
    "test.info()\n",
    "\n",
    "#1. EXPLORATORY DATA ANALYSIS (EDA) \n",
    "\n",
    "# 1.1. Missing Values\n",
    "\n",
    "train.isnull().mean()\n",
    "\n",
    "test.isnull().mean()\n",
    "\n",
    "#There are a lot of missing values in both in **`train`** and **`test`** set. The features having missing values are :\n",
    "* Applicant_City_PIN\n",
    "* Applicant_Gender\n",
    "* Applicant_BirthDate\n",
    "* Applicant_Marital_Status\n",
    "* Applicant_Occupation\n",
    "* Applicant_Qualification\n",
    "* Manager_DOJ\n",
    "* Manager_Joining_Designation\n",
    "* Manager_Current_Designation\n",
    "* Manager_Grade\n",
    "* Manager_Status                 \n",
    "* Manager_Gender                 \n",
    "* Manager_DoB                    \n",
    "* Manager_Num_Application        \n",
    "* Manager_Num_Coded              \n",
    "* Manager_Business               \n",
    "* Manager_Num_Products           \n",
    "* Manager_Business2              \n",
    "* Manager_Num_Products2 #\n",
    "\n",
    "#1.2. Target Variable Distribution\n",
    "\n",
    "#The dataset is almost balanced with the classes, 0 having **65.7 %** records and 1 having **34.3%** records.\n",
    "\n",
    "train['Business_Sourced'].value_counts(normalize=True)\n",
    "\n",
    "sns.countplot(x='Business_Sourced', data=train)\n",
    "\n",
    "#1.3. Numerical Feature Distribution\n",
    "\n",
    "#The Numerical Features in the **`train`** are :\n",
    "* Manager_Num_Application\n",
    "* Manager_Num_Coded\n",
    "* Manager_Business\n",
    "* Manager_Num_Products\n",
    "* Manager_Business2\n",
    "* Manager_Num_Products2\n",
    "\n",
    "#1.3.1. Univariate Analysis\n",
    "\n",
    "def UNI_numerical(df, num_feat):\n",
    "    \n",
    "    cols = len(num_feat)\n",
    "    plt.figure(figsize = (7*cols,3), dpi = 100)\n",
    "    \n",
    "    for idx , feat in enumerate(num_feat) :\n",
    "        \n",
    "        # calculating descriptive statistics values of a feature\n",
    "        min_val = df[feat].min()\n",
    "        max_val = df[feat].max()\n",
    "        ran = max_val - min_val\n",
    "        \n",
    "        mean_val = df[feat].mean()\n",
    "        median_val = df[feat].median()\n",
    "        std_dev = df[feat].std()\n",
    "        skew = df[feat].skew()\n",
    "        kurt = df[feat].kurtosis()\n",
    "        \n",
    "        # Calculating points of standard deviation\n",
    "        std_points = mean_val+std_dev , mean_val-std_dev\n",
    "        \n",
    "        # PLotting the feature with the information\n",
    "        plt.subplot(1, cols+1, idx+1)\n",
    "        sns.kdeplot(x=feat, data=train, shade=True)\n",
    "        sns.lineplot(x=std_points, y=[0,0], color = 'black', label = \"std_dev\")\n",
    "        sns.scatterplot(x=[min_val,max_val], y=[0,0], color = 'orange', label = \"min/max\")\n",
    "        sns.scatterplot(x=[mean_val], y=[0], color = 'red', label = \"mean\")\n",
    "        sns.scatterplot(x=[median_val], y=[0], color = 'blue', label = \"median\")\n",
    "        plt.xlabel('{}'.format(feat), fontsize = 20)\n",
    "        plt.ylabel('density')\n",
    "        plt.title('std_dev = {}; kurtosis = {};\\nskew = {}; range = {}\\nmean = {}; median = {}'.format((round(std_points[0],2),round(std_points[1],2)),\n",
    "                                                                                                   round(kurt,2),\n",
    "                                                                                                   round(skew,2),\n",
    "                                                                                                   (round(min_val,2),round(max_val,2),round(ran,2)),\n",
    "                                                                                                   round(mean_val,2),\n",
    "                                                                                                   round(median_val,2)))\n",
    "\n",
    "#__Train Dataset__\n",
    "\n",
    "UNI_numerical(train, ['Manager_Num_Application','Manager_Num_Coded', 'Manager_Business'])\n",
    "\n",
    "UNI_numerical(train, ['Manager_Num_Products','Manager_Business2', 'Manager_Num_Products2'])\n",
    "\n",
    "#__Test Dataset__\n",
    "\n",
    "UNI_numerical(test, ['Manager_Num_Application','Manager_Num_Coded', 'Manager_Business'])\n",
    "\n",
    "UNI_numerical(test, ['Manager_Num_Products','Manager_Business2', 'Manager_Num_Products2'])\n",
    "\n",
    "#1.3.2. Outliers\n",
    "\n",
    "def outliers_detection(df, num_feats):\n",
    "    \n",
    "    plt.figure(figsize=(7*len(num_feats), 4))\n",
    "    \n",
    "    for index, feat in enumerate(num_feats) :\n",
    "        plt.subplot(1, 3, index+1)\n",
    "        sns.boxplot(train[feat])\n",
    "        plt.xlabel('{}'.format(feat), fontsize = 20)\n",
    "    \n",
    "    return\n",
    "\n",
    "#__Train Dataset__\n",
    "\n",
    "outliers_detection(train, ['Manager_Num_Application','Manager_Num_Coded', 'Manager_Business'])\n",
    "\n",
    "outliers_detection(train, ['Manager_Num_Products','Manager_Business2', 'Manager_Num_Products2'])\n",
    "\n",
    "#__Test Dataset__\n",
    "\n",
    "outliers_detection(train, ['Manager_Num_Application','Manager_Num_Coded', 'Manager_Business'])\n",
    "\n",
    "outliers_detection(train, ['Manager_Num_Products','Manager_Business2', 'Manager_Num_Products2'])\n",
    "\n",
    "#1.4. Correlations\n",
    "\n",
    "numerical_features = [\"Manager_Num_Application\", \"Manager_Num_Coded\", \"Manager_Business\", \"Manager_Num_Products\",\n",
    "\"Manager_Business2\", \"Manager_Num_Products2\", \"Business_Sourced\"]\n",
    "\n",
    "correlation_train = train[numerical_features].corr(method = 'pearson')\n",
    "plt.title(\"Train Dataset Correlations\", fontsize=20)\n",
    "sns.heatmap(correlation_train, linewidth = 4, annot=True, cmap='plasma')\n",
    "\n",
    "#The features **Manager_Business** and **Manager_Business2** are highly coorelated. Similarly a high correlation is observed between **Manager_Num_Products** and **Manager_Num_Products2**. <br>\n",
    "#In order to remove multi-colinearity the columns **Manager_Business2** and **Manager_Num_Products2** will be dropped.\n",
    "\n",
    "\n",
    "#As expected there will be a **strong correlation** between **Manager_Num_Products** and **Manager_Business**. \n",
    "\n",
    "\n",
    "# **As the number of products sold increases the amount of business sourced will also increase.**\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x='Manager_Business', y='Manager_Num_Products', data=train, hue='Business_Sourced')\n",
    "\n",
    "#1.5. Categorical Distribution\n",
    "\n",
    "categorical_features = [\"Applicant_Gender\", \"Applicant_Marital_Status\",\"Applicant_Occupation\", \n",
    "                    \"Applicant_Qualification\", \"Manager_Joining_Designation\",\"Manager_Current_Designation\"\n",
    "                    ,\"Manager_Grade\",\"Manager_Status\",\"Manager_Gender\"]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, nrows=3, figsize=(20,20))\n",
    "plt.subplots_adjust(right=1.5, wspace=0.4, hspace=0.4)\n",
    "\n",
    "for i , col in enumerate(categorical_features) :\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    # Distribution of Is Lead in feature\n",
    "    sns.countplot(x= col, data = train, hue='Business_Sourced')\n",
    "    plt.xlabel('')\n",
    "    plt.ylabel('')\n",
    "    plt.tick_params(axis='x', labelsize=20)\n",
    "    plt.tick_params(axis='y', labelsize=20)\n",
    "    plt.legend(loc='upper right', prop={'size': 20})\n",
    "    plt.title('Count of Business_Sourced in {}'.format(col), size=20, y=1.05)\n",
    "        \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.countplot(x='Applicant_Qualification', data=train, hue='Business_Sourced')\n",
    "plt.tick_params(axis='x', labelrotation=45)\n",
    "plt.title(\"Count of Business Sourced per Applicant Qualification Group\", fontsize=25)\n",
    "plt.legend(loc='upper right')\n",
    "plt.xlabel(\"Applicant's Qualification\")\n",
    "\n",
    "#1.6. DataType of features\n",
    "\n",
    "#The data type of the date columns **'Application_Receipt_Date', 'Applicant_BirthDate', 'Manager_DoB', 'Manager_DOJ'** are not in datetime. \n",
    "#The datatype conversion of the columns needs to be done in the data cleaning step.\n",
    "\n",
    "train.info()\n",
    "\n",
    "train['Application_Receipt_Date'] = pd.to_datetime(train['Application_Receipt_Date'])\n",
    "test['Application_Receipt_Date'] = pd.to_datetime(test['Application_Receipt_Date'])\n",
    "\n",
    "train.info()\n",
    "\n",
    "test.info()\n",
    "\n",
    "# 1.7. Insights around the features\n",
    "\n",
    "# Dictionary containing the month names\n",
    "\n",
    "Calender_Month_Name = {\n",
    "    1: 'Jan',\n",
    "    2: 'Feb',\n",
    "    3: 'Mar',\n",
    "    4: 'Apr',\n",
    "    5: 'May',\n",
    "    6: 'Jun',\n",
    "    7: 'Jul',\n",
    "    8: 'Aug',\n",
    "    9: 'Sep',\n",
    "    10: 'Oct',\n",
    "    11: 'Nov',\n",
    "    12: 'Dec'\n",
    "}\n",
    "\n",
    "#1.7.1. Number of applications received per Application_Receipt_Date\n",
    "\n",
    "# **CONCLUSION**\n",
    "#The peak number of applications were received in the month of **May, 2007**. In initial months the number of applicatins received was low. However the number increased in the subsequent months.**The a huge bulk of applications are received in the months starting from July till December in both the years of 2007 and 2008**\n",
    "\n",
    "# Number of applications received per day. (More Granular data at day level)\n",
    "\n",
    "num_applications_per_day = train.groupby(['Application_Receipt_Date'])['ID'].count().reset_index()\n",
    "num_applications_per_day['App_Year'] = num_applications_per_day['Application_Receipt_Date'].dt.year\n",
    "num_applications_per_day['App_Month'] = num_applications_per_day['Application_Receipt_Date'].dt.month\n",
    "num_applications_per_day['App_Day'] = num_applications_per_day['Application_Receipt_Date'].dt.day\n",
    "num_applications_per_day['App_Date_Num'] = num_applications_per_day['App_Year']*10000+num_applications_per_day['App_Month']*100+num_applications_per_day['App_Day']\n",
    "\n",
    "num_applications_per_day.head()\n",
    "num_applications_per_day.tail()\n",
    "\n",
    "# Number of applications received per month-year. (More Granular data at day level)\n",
    "\n",
    "num_app_month_year = num_applications_per_day.groupby(['App_Year', 'App_Month'])['ID'].sum().reset_index()\n",
    "num_app_month_year['Month_Year'] = num_app_month_year['App_Year']*100+num_app_month_year['App_Month']\n",
    "num_app_month_year\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.barplot(x=\"Month_Year\", y=\"ID\", data=num_app_month_year)\n",
    "plt.title(\"Number of Applications received per Month\", fontsize=15)\n",
    "plt.xlabel(\"Month - Year\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Lookup table for plotting each year, month then day level data\n",
    "\n",
    "app_year_month = num_applications_per_day[['App_Year', 'App_Month']].drop_duplicates()\n",
    "app_year_month.shape\n",
    "\n",
    "# Position of the plots for 16 months data covering both 2007 and 2008...\n",
    "\n",
    "plot_locs = [[0,0], [0,1], [0,2], [0,3],\n",
    "             [1,0], [1,1], [1,2], [1,3],\n",
    "             [2,0], [2,1], [2,2], [2,3],\n",
    "             [3,0], [3,1], [3,2], [3,3]]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(30,30))\n",
    "fig.subplots_adjust(wspace=0.2,  hspace=0.2)\n",
    "\n",
    "print(\"Number of applications received at a day level\")\n",
    "print()\n",
    "\n",
    "for i , row in enumerate(app_year_month.iterrows()):\n",
    "    \n",
    "    year = row[1][0]\n",
    "    month = row[1][1]\n",
    "    \n",
    "    num_app_at_year_month = num_applications_per_day[\n",
    "        (num_applications_per_day['App_Year']==year) & (num_applications_per_day['App_Month']==month)]\n",
    "    \n",
    "    plot_x = plot_locs[i][0]\n",
    "    plot_y = plot_locs[i][1]\n",
    "    \n",
    "    sns.barplot(x=\"App_Day\", y=\"ID\", data=num_app_at_year_month, ax=axs[plot_x][plot_y])\n",
    "    axs[plot_x][plot_y].set_title(\"{}, {}\".format(Calender_Month_Name.get(month),year), fontsize=20)\n",
    "    axs[plot_x][plot_y].tick_params(axis='x', labelrotation=45, labelsize=12)\n",
    "    axs[plot_x][plot_y].set_ylabel(\" \")\n",
    "    axs[plot_x][plot_y].set_xlabel(\"Application Receipt Day\")\n",
    "\n",
    "#We have an idea about the number of applications received per day and the trend of the volumne of applications received throughout the year.<br> \n",
    "\n",
    "#1.7.2. Number of Products sold by the Manager in last 3 months at the time of Application_Receipt_Date\n",
    "\n",
    "# **AIM - To investigate the trend of the number of products sold by the Manager in the last 3 months at the time of the application receipt date, to get an idea of the sucessfull and relatively failure months of business.** \n",
    "\n",
    "Sales = train.groupby(['Application_Receipt_Date', 'Business_Sourced'])['Manager_Num_Products'].sum().reset_index()\n",
    "\n",
    "# Year, Month and Day extraction from Application_Receipt_Date.\n",
    "Sales['App_Year'] = Sales['Application_Receipt_Date'].dt.year\n",
    "Sales['App_Month'] = Sales['Application_Receipt_Date'].dt.month\n",
    "Sales['App_Day'] = Sales['Application_Receipt_Date'].dt.day\n",
    "Sales['App_Date_Num'] = Sales['App_Year']*10000+Sales['App_Month']*100+Sales['App_Day']\n",
    "Sales['App_Day'] = Sales['App_Day'].astype('object')\n",
    "Sales.head()\n",
    "\n",
    "# Position of the plots for 16 months data covering both 2007 and 2008...\n",
    "\n",
    "plot_locs = [[0,0], [0,1], [0,2], [0,3],\n",
    "             [1,0], [1,1], [1,2], [1,3],\n",
    "             [2,0], [2,1], [2,2], [2,3],\n",
    "             [3,0], [3,1], [3,2], [3,3]]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(30,30))\n",
    "fig.subplots_adjust(wspace=0.2,  hspace=0.2) \n",
    "\n",
    "\n",
    "for i , row in enumerate(app_year_month.iterrows()):\n",
    "    \n",
    "    year = row[1][0]\n",
    "    month = row[1][1]\n",
    "    \n",
    "    sales_at_year_month = Sales[(Sales['App_Year']==year) & (Sales['App_Month']==month)]\n",
    "    \n",
    "    plot_x = plot_locs[i][0]\n",
    "    plot_y = plot_locs[i][1]\n",
    "    \n",
    "    sns.lineplot(x=\"App_Day\", y=\"Manager_Num_Products\", data=sales_at_year_month, hue=\"Business_Sourced\", ax=axs[plot_x][plot_y])\n",
    "    axs[plot_x][plot_y].set_title(\"{}, {}\".format(Calender_Month_Name.get(month),year), fontsize=20)\n",
    "    axs[plot_x][plot_y].tick_params(axis='x', labelsize=10)\n",
    "    axs[plot_x][plot_y].set_xlabel(\"Application Receipt Day\")\n",
    "    axs[plot_x][plot_y].set_ylabel(\" \")\n",
    "\n",
    "# **CONCLUSION**\n",
    "#Initially in the period of Apr - Aug 2007, the number of products sold where business was sourced is very less than the times when the business was not sourced. The number of products sold where business was sourced started to increase in September, 2007.<br>\n",
    "\n",
    "#The difference between the number of products sold between busniess sourced and non-soucred gradually decreased and this trend continued till March, 2008. There were instances where Number of products sold when business was sourced is more than that when not sourced.<br>\n",
    "\n",
    "#1.7.3. Status of applications per Application_Receipt_Date\n",
    "\n",
    "#**AIM - To investigate each application received throughout the time period, whether the agent was able to source business within 3 months after his / her training.** \n",
    "\n",
    "# Business_Sourced = 1 for Business Sourced and 0 for not sourced\n",
    "\n",
    "Application_0_1 = train[['ID', 'Application_Receipt_Date', 'Business_Sourced']]\n",
    "\n",
    "# Calculating the sequence / record row number in which the application was received for each Application Receipt Date.\n",
    "\n",
    "Application_0_1 = pd.concat([Application_0_1, Application_0_1.groupby(['Application_Receipt_Date']).cumcount()+1], \n",
    "                            axis=1)\n",
    "Application_0_1.rename(columns={0: \"App_Order\"}, inplace=True)\n",
    "\n",
    "# Year, Month and Day extraction from Application_Receipt_Date.\n",
    "\n",
    "Application_0_1['App_Year'] = Application_0_1['Application_Receipt_Date'].dt.year\n",
    "Application_0_1['App_Month'] = Application_0_1['Application_Receipt_Date'].dt.month\n",
    "Application_0_1['App_Day'] = Application_0_1['Application_Receipt_Date'].dt.day\n",
    "Application_0_1['App_Date_Num'] = Application_0_1['App_Year']*10000+Application_0_1['App_Month']*100+Application_0_1['App_Day']\n",
    "\n",
    "Application_0_1.head()\n",
    "\n",
    "# Position of the plots 16 months data covering both 2007 and 2008...\n",
    "\n",
    "plot_locs = [[0,0], [0,1], [0,2], [0,3],\n",
    "             [1,0], [1,1], [1,2], [1,3],\n",
    "             [2,0], [2,1], [2,2], [2,3],\n",
    "             [3,0], [3,1], [3,2], [3,3]]\n",
    "\n",
    "fig, axs = plt.subplots(ncols=4, nrows=4, figsize=(30,30))\n",
    "fig.subplots_adjust(wspace=0.2,  hspace=0.2) \n",
    "\n",
    "\n",
    "for i , row in enumerate(app_year_month.iterrows()):\n",
    "    \n",
    "    year = row[1][0]\n",
    "    month = row[1][1]\n",
    "    \n",
    "    app_at_year_month = Application_0_1[(Application_0_1['App_Year']==year) & (Application_0_1['App_Month']==month)]\n",
    "    \n",
    "    plot_x = plot_locs[i][0]\n",
    "    plot_y = plot_locs[i][1]\n",
    "    \n",
    "    sns.scatterplot(x=\"App_Order\", y=\"App_Day\", data=app_at_year_month, hue=\"Business_Sourced\", ax=axs[plot_x][plot_y])\n",
    "    axs[plot_x][plot_y].set_title(\"{}, {}\".format(Calender_Month_Name.get(month),year), fontsize=20)\n",
    "    axs[plot_x][plot_y].tick_params(axis='x', labelsize=20)\n",
    "    axs[plot_x][plot_y].set_xlabel(\" \")\n",
    "    axs[plot_x][plot_y].set_ylabel(\"Application Receipt Day\")\n",
    "\n",
    "# **CONCLUSION** A trend is captured. For a particular day the agent's application which was received first or relatively at the beginning of the day was able to source business within 3 months post 7 day training. This pattern is observed across all the 16 months of the train dataset. \n",
    "\n",
    "\n",
    "#1.7.4. Business from Category A advisor\n",
    "\n",
    "#The business from **`Category A advisor`** will be the difference between the **`Manager_Business`** and **`Manager_Business2`** since,\n",
    "#- **`Manager_Business`** is the amount of business sourced by the manager in last 3 months, \n",
    "#- **`Manager_Business2`** is the amount of business sourced by the manager in last 3 months excluding business from their Category A advisor\n",
    "\n",
    "#The number of products sold by **`Category A advisor`** in the last 3 months will be the difference between the **`Manager_Num_Products`** and **`Manager_Num_Products2`** since,\n",
    "#- **`Manager_Num_Products`** is Number of products sold by the manager in last 3 months\n",
    "#- **`Manager_Num_Products2`** is the number of products sold by the manager in last 3 months excluding business from their Category A advisor\n",
    "\n",
    "Category_A_Business = train[['Manager_Business', 'Manager_Business2', 'Manager_Num_Products', 'Manager_Num_Products2']]\n",
    "\n",
    "Category_A_Business['Num_Products_Category_A'] = Category_A_Business['Manager_Num_Products'] - Category_A_Business['Manager_Num_Products2']\n",
    "Category_A_Business['Business_Sourced_Category_A'] = Category_A_Business['Manager_Business'] - Category_A_Business['Manager_Business2']\n",
    "\n",
    "Category_A_Business.head()\n",
    "\n",
    "Category_A_Business[['Business_Sourced_Category_A', 'Num_Products_Category_A']].describe()\n",
    "\n",
    "Category_A_Business[(Category_A_Business['Manager_Business'] < Category_A_Business['Manager_Business2'])]\n",
    "\n",
    "#**CONCLUSION-** The features **Manager_Business** and **Manager_Business2** are highly coorelated. Similarly a high correlation is observed between **Manager_Num_Products** and **Manager_Num_Products2**. <br>\n",
    "\n",
    "#In order to remove multi-colinearity the columns **Manager_Business2** and **Manager_Num_Products2** will be dropped.\n",
    "\n",
    "#2. DATA PREPROCESSING \n",
    "\n",
    "# 19 features out of 23 had missing values.\n",
    "\n",
    "\n",
    "# The Arbitray Value imputation is done for handling missing values in the numerical, categorical and date columns / features.\n",
    "\n",
    "\n",
    "# Datatype conversion - The date columns were converted to proper datetime data type.\n",
    "\n",
    "\n",
    "# Irrelevent features were dropped from train and test datasets.\n",
    "\n",
    "train.isnull().sum()\n",
    "\n",
    "#2.1. Missing Value Imputation\n",
    "\n",
    "# Filling missing values of numerical features with an arbitrary value.\n",
    "\n",
    "numerical_cols = ['Manager_Num_Application', 'Manager_Num_Coded', 'Manager_Business' , 'Manager_Num_Products', \n",
    "            'Manager_Business2', 'Manager_Num_Products2', 'Applicant_City_PIN']\n",
    "\n",
    "for col in numerical_cols :\n",
    "    train[col] = train[col].fillna(-999)\n",
    "    test[col] = test[col].fillna(-999)\n",
    "\n",
    "# Filling missing values of date features with an arbitrary value.\n",
    "\n",
    "date_cols = ['Application_Receipt_Date', 'Applicant_BirthDate', 'Manager_DoB', 'Manager_DOJ']\n",
    "\n",
    "for date_col in date_cols :\n",
    "    train[date_col] = train[date_col].fillna(\"1/1/1900\")\n",
    "    test[date_col] = test[date_col].fillna(\"1/1/1900\")\n",
    "\n",
    "# Filling missing values of categorical features with an arbitrary value.\n",
    "\n",
    "agent_manager_details = ['Applicant_City_PIN', 'Applicant_Marital_Status', 'Applicant_Occupation',\n",
    "                         'Applicant_Qualification', 'Manager_Joining_Designation', 'Manager_Current_Designation',\n",
    "                         'Manager_Grade', 'Manager_Status', 'Manager_Gender']\n",
    "\n",
    "for feat in agent_manager_details :\n",
    "    train[feat] = train[feat].fillna(\"Missing\")\n",
    "    test[feat] = test[feat].fillna(\"Missing\")\n",
    "\n",
    "train['Applicant_Gender'] = train['Applicant_Gender'].fillna('Gender_NA')\n",
    "test['Applicant_Gender'] = test['Applicant_Gender'].fillna('Gender_NA')\n",
    "\n",
    "#2.2. Data Type Conversion\n",
    "\n",
    "# Converting date features from object to datetime to create new features from them in the feature engineering step.\n",
    "\n",
    "date_cols = ['Applicant_BirthDate', 'Manager_DoB', 'Manager_DOJ']\n",
    "\n",
    "for date_col in date_cols :\n",
    "    train[date_col] = pd.to_datetime(train[date_col])\n",
    "    test[date_col] = pd.to_datetime(test[date_col])\n",
    "\n",
    "# Checking for any remaining features with missing values.\n",
    "\n",
    "train.isnull().sum()\n",
    "\n",
    "train.info()\n",
    "\n",
    "#3. Feature Engineering\n",
    "\n",
    "# 4 extra numerical features were created :\n",
    "#1. Agent_Age : The age of the Applicant / agent as on Application Receipt Date.\n",
    "#2. Manager_Age : The age of the Manager as on Application Receipt Date.\n",
    "#3. Manager_Exp : The work experience of Manager in the company.\n",
    "#4. App_Order_Percent : Percentile of the position of the Application Received calculated at a daily level.\n",
    "\n",
    "#- The categorical features (Applicant_Gender, Applicant_Occupation) were One Hot Encoded.\n",
    "\n",
    "#- (Manager_Joining_Designation, Manager_Current_Designation) were Label Encoded.\n",
    "\n",
    "#Creating a function in order to build distribution plots in both train and test dataset for \n",
    "#3.1. Agent Age\n",
    "#3.2. Manager Age\n",
    "#3.3. Manager's Experience\n",
    "\n",
    "def distribution_plot(feat):\n",
    "    \n",
    "    fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(8,5))\n",
    "    fig.subplots_adjust(right=1.5)\n",
    "\n",
    "    sns.distplot(x=train[feat], ax=axs[0])\n",
    "    axs[0].set_title(\"{} Distribution in train dataset\".format(feat), fontsize=15)\n",
    "\n",
    "    sns.distplot(x=test[feat], ax=axs[1])\n",
    "    axs[1].set_title(\"{} Distribution in test dataset\".format(feat), fontsize=15)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    return\n",
    "\n",
    "#3.1. Age of the Applicant / Agent\n",
    "\n",
    "train['Agent_Age'] = ((train['Application_Receipt_Date'] - train['Applicant_BirthDate'])/np.timedelta64(1, 'D'))/365\n",
    "test['Agent_Age'] = ((test['Application_Receipt_Date'] - test['Applicant_BirthDate'])/np.timedelta64(1, 'D'))/365\n",
    "\n",
    "distribution_plot(\"Agent_Age\")\n",
    "\n",
    "#3.2. Age of the Manager\n",
    "\n",
    "train['Manager_Age'] = ((train['Application_Receipt_Date'] - train['Manager_DoB'])/np.timedelta64(1, 'D'))/365\n",
    "test['Manager_Age'] = ((test['Application_Receipt_Date'] - test['Manager_DoB'])/np.timedelta64(1, 'D'))/365\n",
    "\n",
    "distribution_plot(\"Manager_Age\")\n",
    "\n",
    "#3.3. Manager's Experience in FinMan\n",
    "\n",
    "train['Manager_Exp'] = ((train['Application_Receipt_Date'] - train['Manager_DOJ'])/np.timedelta64(1, 'D'))/365\n",
    "test['Manager_Exp'] = ((test['Application_Receipt_Date'] - test['Manager_DOJ'])/np.timedelta64(1, 'D'))/365\n",
    "\n",
    "distribution_plot(\"Manager_Exp\")\n",
    "\n",
    "#3.4. App_Order_percent\n",
    "\n",
    "#Creating a function to check the relationship of the Application sequence w.r.t Application Receipt and sourced business\n",
    "def App_Seq(df):\n",
    "    \n",
    "    cols= ['ID', 'Application_Receipt_Date']\n",
    "    \n",
    "    Applications = df[cols]\n",
    "    Applications = pd.concat([Applications, Applications.groupby(['Application_Receipt_Date']).cumcount()+1], axis=1)\n",
    "    Applications.rename(columns={0: \"App_Order\"}, inplace=True)\n",
    "    max_app_seq = Applications.groupby(['Application_Receipt_Date'])['App_Order'].max().reset_index()\n",
    "    min_app_seq = Applications.groupby(['Application_Receipt_Date'])['App_Order'].min().reset_index()\n",
    "\n",
    "    max_min_seq = pd.merge(max_app_seq, min_app_seq , how='inner' , on='Application_Receipt_Date')\n",
    "    max_min_seq.rename(columns={'App_Order_x' : 'Max_Seq', 'App_Order_y' : 'Min_Seq'}, inplace=True)\n",
    "\n",
    "    max_min_seq['Range_Seq'] = max_min_seq['Max_Seq'] - max_min_seq['Min_Seq']\n",
    "    Applications = pd.merge(Applications, max_min_seq, how='inner' , on='Application_Receipt_Date')\n",
    "    \n",
    "    Applications['App_Order_percent'] = (Applications['App_Order'] - Applications['Min_Seq'])/Applications['Range_Seq']\n",
    "    \n",
    "    \n",
    "    df = pd.merge(df, Applications.drop(columns=['App_Order', 'Application_Receipt_Date', 'Max_Seq', 'Min_Seq',\n",
    "                                                 'Range_Seq']) , how='inner', on='ID')\n",
    "    \n",
    "    return df, Applications\n",
    "\n",
    "train, App_data_train = App_Seq(train)\n",
    "test, App_data_test = App_Seq(test)\n",
    "\n",
    "train.head()\n",
    "\n",
    "#**CONCLUSION- The Applications placed at first in the sequence per Application Receipt Date are more likely to have sourced business**\n",
    "\n",
    "print(train[train['Business_Sourced']==1]['App_Order_percent'].mean())\n",
    "\n",
    "#__Handling Missing Values in App_Order_Percent Feature__\n",
    "\n",
    "train[train['App_Order_percent'].isnull()]\n",
    "\n",
    "test[test['App_Order_percent'].isnull()]\n",
    "\n",
    "train['App_Order_percent'] = train['App_Order_percent'].fillna(0.00)\n",
    "test['App_Order_percent'] = test['App_Order_percent'].fillna(0.00)\n",
    "\n",
    "train.columns\n",
    "\n",
    "train.info()\n",
    "\n",
    "#3.5. Categorical Encodings\n",
    "\n",
    "#3.5.1. Applicant Gender\n",
    "\n",
    "train['Applicant_Gender'].unique()\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train['Applicant_Gender'], drop_first=True)], axis=1)\n",
    "test = pd.concat([test, pd.get_dummies(test['Applicant_Gender'], drop_first=True)], axis=1)\n",
    "\n",
    "train.info()\n",
    "\n",
    "train.drop(columns=['Applicant_Gender'], inplace=True)\n",
    "test.drop(columns=['Applicant_Gender'], inplace=True)\n",
    "\n",
    "#3.5.2. Manager Joining Designation\n",
    "\n",
    "# Label Encoding of Joining and Current Designation Levels\n",
    "\n",
    "Designation_Level = {\n",
    "    'Level 1' : 1, \n",
    "    'Level 2' : 2, \n",
    "    'Level 3' : 3,\n",
    "    'Level 4' : 4,\n",
    "    'Level 5' : 5,\n",
    "    'Level 6' : 6,\n",
    "    'Level 7' : 7,\n",
    "    'Missing' : -1, \n",
    "    'Other' : -1\n",
    "}\n",
    "\n",
    "train['Manager_Joining_Designation'] = train['Manager_Joining_Designation'].map(Designation_Level)\n",
    "test['Manager_Joining_Designation'] = test['Manager_Joining_Designation'].map(Designation_Level)\n",
    "\n",
    "#3.5.3. Manager Current Designation\n",
    "\n",
    "train['Manager_Current_Designation'] = train['Manager_Current_Designation'].map(Designation_Level)\n",
    "test['Manager_Current_Designation'] = test['Manager_Current_Designation'].map(Designation_Level)\n",
    "\n",
    "#3.5.4. Applicant Occupation\n",
    "\n",
    "train['Applicant_Occupation'].unique()\n",
    "\n",
    "train = pd.concat([train, pd.get_dummies(train['Applicant_Occupation'])] , axis=1)\n",
    "train.head()\n",
    "\n",
    "test = pd.concat([test, pd.get_dummies(test['Applicant_Occupation'])] , axis=1)\n",
    "\n",
    "#4. Droping Irrelevent Columns\n",
    "\n",
    "irrelevent_cols = [\"ID\", \"Applicant_BirthDate\", \"Applicant_Marital_Status\", \"Applicant_Qualification\", \"Manager_DOJ\",\n",
    "\"Manager_Grade\", \"Manager_Status\", \"Manager_Gender\", \"Manager_DoB\", \"Manager_Business2\", \"Manager_Num_Products2\",\n",
    "                  \"Application_Receipt_Date\", \"Applicant_Occupation\"]\n",
    "\n",
    "train.drop(columns=irrelevent_cols, inplace=True)\n",
    "\n",
    "df_test = test.copy()\n",
    "\n",
    "test.drop(columns=irrelevent_cols, inplace=True)\n",
    "\n",
    "train.info()\n",
    "\n",
    "test.info()\n",
    "\n",
    "plt.figure(figsize=(15,15))\n",
    "correlation = train.corr(method=\"pearson\")\n",
    "sns.heatmap(correlation, annot=True, cmap=\"viridis\")\n",
    "\n",
    "#5. Modelling\n",
    "\n",
    "# Spliting target variable from train dataset\n",
    "\n",
    "X = train.drop(columns=['Business_Sourced'])\n",
    "y = train[['Business_Sourced']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)\n",
    "\n",
    "#5.1. XGBoost Model\n",
    "\n",
    "xgb_model = XGBClassifier(objective=\"binary:logistic\", n_estimators=200, learning_rate=0.05, max_depth=6, \n",
    "                          subsample=0.75, colsample_bytree=0.8, min_child_weight=1, n_jobs=-1, random_state=42,\n",
    "                          eval_metric=\"auc\") \n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train,y_train)\n",
    "proba = xgb_model.predict_proba(X_train)[:,1]\n",
    "train_score = roc_auc_score(y_train,proba)\n",
    "cv_score = cross_val_score(xgb_model,X_train,y_train,scoring=\"roc_auc\",verbose=2,cv =skf)\n",
    "\n",
    "print('Train score : {}', train_score)\n",
    "print('CV Scores : {}', cv_score)\n",
    "print('Mean CV Scores : {}, Variance in CV Scores : {}'.format(cv_score.mean(), cv_score.std()))\n",
    "\n",
    "xgb_model.fit(X_train,y_train)\n",
    "proba = xgb_model.predict_proba(X_test)[:,1]\n",
    "test_score = roc_auc_score(y_test,proba)\n",
    "print('Test score : {}', test_score)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "plot_importance(xgb_model , ax=ax)\n",
    "\n",
    "#In the XGBoost model, the top 5 features of importance are : \n",
    "#Agent_Age, App_Order_percent, Manager_Age, Applicant_City_PIN and Manager_Exp.\n",
    "\n",
    "xgb_model.fit(X,y)\n",
    "\n",
    "pred = xgb_model.predict_proba(test)[:,1]\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['ID'] = df_test['ID']\n",
    "submission[\"Business_Sourced\"] = pred\n",
    "\n",
    "submission.to_csv('xgb_predictions_cv_Sami1.csv', index=False)\n",
    "\n",
    "#5.2. Light Gradient Boosting Model\n",
    "\n",
    "param_dist = {\"n_estimators\":sp_randint(40,80),\n",
    "              \"colsample_bytree\":np.array([0.5,0.6,0.7,0.8,0.9,1]),\n",
    "              \"subsample\":np.array([0.5,0.6,0.7,0.8,0.9,1]),\n",
    "              \"reg_lambda\":np.array([1e-5,1e-4,1e-3,1e-2,0.1,1,10,100]),\n",
    "              \"reg_alpha\":np.array([1e-5,1e-4,1e-3,1e-2,0.1,1,10,100]),\n",
    "              \"min_child_samples\": sp_randint(25,65),\n",
    "                \"max_depth\": sp_randint(1,20)}\n",
    "\n",
    "clf_lgbm = LGBMClassifier(boosting_type = \"gbdt\",n_jobs =-1,random_state = 42)\n",
    "\n",
    "# Randomized Search CV for finding the best parameters under roc_auc scoring\n",
    "\n",
    "lgbm_cv = RandomizedSearchCV(clf_lgbm, param_distributions=param_dist,\n",
    "                                   n_iter=50,cv=skf,scoring='roc_auc',random_state=42,verbose=1)\n",
    "\n",
    "lgbm_cv.fit(X_train,y_train)\n",
    "\n",
    "print('mean test scores',lgbm_cv.cv_results_['mean_test_score'])\n",
    "print(lgbm_cv.cv_results_['mean_test_score'].mean(), lgbm_cv.cv_results_['mean_test_score'].std())\n",
    "\n",
    "print('The best parameters {}'.format(lgbm_cv.best_params_))\n",
    "\n",
    "lgb_clf_model = LGBMClassifier(colsample_bytree=0.4, max_depth=12, min_child_samples=49,\n",
    "               n_estimators=59, random_state=42, reg_alpha=1.0,\n",
    "               reg_lambda=0.001, subsample=1.0)\n",
    "\n",
    "lgb_clf_model.fit(X_train,y_train)\n",
    "proba = lgb_clf_model.predict_proba(X_train)[:,1]\n",
    "train_score = roc_auc_score(y_train,proba)\n",
    "cv_score = cross_val_score(lgb_clf_model,X_train,y_train,scoring=\"roc_auc\",verbose=2,cv =skf)\n",
    "\n",
    "print('Train score : {}', train_score)\n",
    "print('CV Scores : {}', cv_score)\n",
    "print('Mean CV Scores : {}, Variance in CV Scores : {}'.format(cv_score.mean(), cv_score.std()))\n",
    "\n",
    "lgb_clf_model.fit(X_train,y_train)\n",
    "proba = lgb_clf_model.predict_proba(X_test)[:,1]\n",
    "test_score = roc_auc_score(y_test,proba)\n",
    "print('Test score : {}', test_score)\n",
    "\n",
    "lgb_clf_model.fit(X,y)\n",
    "\n",
    "pred = lgb_clf_model.predict_proba(test)[:,1]\n",
    "lgbm_submission = pd.DataFrame()\n",
    "lgbm_submission['ID'] = df_test['ID']\n",
    "lgbm_submission[\"Business_Sourced\"] = pred\n",
    "\n",
    "lgbm_submission.to_csv('lgbm_pred_cv_Sami3.csv', index=False)\n",
    "\n",
    "feature_imp = pd.DataFrame(sorted(zip(lgb_clf_model.feature_importances_,X.columns)), columns=['Value','Feature'])\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False))\n",
    "plt.title('LightGBM Features (avg over folds)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "#In the LightGBM model, the top 5 features of importance are : \n",
    "#App_Order_percent, Manager_Exp, Applicant_City_PIN, Office_PIN, Manager_Age and Agent_Age.\n",
    "\n",
    "#5.3. Ada Boost Model\n",
    "\n",
    "param_dist = {\"n_estimators\":sp_randint(40,100),\n",
    "              \"learning_rate\":np.array([0.5,0.6,0.7,0.8,0.9,1])}\n",
    "\n",
    "ada_boost_mdl = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "ada_boost_cv = RandomizedSearchCV(ada_boost_mdl, param_distributions=param_dist,\n",
    "                                   n_iter=20,cv=8,scoring='roc_auc',random_state=42,verbose=1)\n",
    "\n",
    "ada_boost_cv.fit(X_train,y_train)\n",
    "\n",
    "print('mean test scores',ada_boost_cv.cv_results_['mean_test_score'])\n",
    "print(ada_boost_cv.cv_results_['mean_test_score'].mean(), ada_boost_cv.cv_results_['mean_test_score'].std())\n",
    "\n",
    "print('The best parameters {}'.format(ada_boost_cv.best_params_))\n",
    "\n",
    "ada_boost = AdaBoostClassifier(learning_rate=0.9, n_estimators=60, random_state=42)\n",
    "\n",
    "ada_boost.fit(X_train,y_train)\n",
    "\n",
    "proba = ada_boost.predict_proba(X_test)[:,1]\n",
    "test_score = roc_auc_score(y_test,proba)\n",
    "\n",
    "print('Train Score : ',test_score)\n",
    "\n",
    "ada_boost.fit(X,y)\n",
    "\n",
    "pred = ada_boost.predict_proba(test)[:,1]\n",
    "ada_boost_submission = pd.DataFrame()\n",
    "ada_boost_submission['ID'] = df_test['ID']\n",
    "ada_boost_submission[\"Business_Sourced\"] = pred\n",
    "\n",
    "ada_boost_submission.to_csv('ada_boost_predictions_Sami1.csv', index=False)\n",
    "\n",
    "# ACCEPTED MODEL -  Light Gradient Boosting (Least Variance in CV scores)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
